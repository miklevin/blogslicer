{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3d280-9787-4d4e-88de-591a7aaa2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614fa7b-91f8-46fb-ab1d-321da61d09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%reload_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499493a1-d63e-4d9a-a751-8282563f4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04c56f-fa5a-489c-8f2e-1ac9fbb65964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# There Are 12 Posts:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### FILE: ../pythonically/_posts/2022-04-29-post-12.md"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "layout: post\n",
      "title: \"Post 12\"\n",
      "categories: blog\n",
      "slug: post-12\n",
      "---\n",
      "\n",
      "Yesterday I got the must-have features of my blogslicer program done. It can be\n",
      "found in github as [blogslicer](https://github.com/miklevin/blogslicer/). This\n",
      "is actually rather big in my life and one of the perfect \"because I'm 50\"\n",
      "projects. It's the first of my ELPGD (every little project gets done)\n",
      "mentality. Honestly, I've spent the last 15 years or so just re-educating\n",
      "myself and getting tech-literate again.\n",
      "\n",
      "- I once was tech-literate because of the Amiga computer. Then it went away.\n",
      "- I again was tech-literate because of Microsoft Active Server Pages. Then it\n",
      "  went away.\n",
      "- I was fooled twice and refused to get fooled again.\n",
      "  - So I passed over Ruby on Rails\n",
      "  - So I passed over JavaScript on the server\n",
      "- After long soul-searching, I settled on:\n",
      "  - Linux\n",
      "  - Python\n",
      "  - vim\n",
      "  - git\n",
      "\n",
      "And I never looked back... oh, except when JavaScript took over the world as\n",
      "\"the full webstack\". Then I looked back, had buyer's remorse, looked more\n",
      "closely at JavaScript and ECMA script, then was happy with my LPvg (Linux,\n",
      "Python, vim & git) decision. Ugh! I see the hamster wheel JavaScript folks are\n",
      "on today and I see it as the institutionalization of the very tech-reset-button\n",
      "I despise. Even if your core language stays the same, it's just as bad if the\n",
      "core language is awful and all the frameworks based on it (out of necessity)\n",
      "are both equally awful and constantly changing.\n",
      "\n",
      "If I need a web-service (or web-anything, for that matter) there's Flask. There\n",
      "always was Flask. There always will be Flask. And while there are many others\n",
      "like FastAPI, pylons and the like, they're all optional higher-dependency\n",
      "fad-driven edge cases that come and go. Flask always remains. It's too big to\n",
      "fail and too widely deployed to not be upgraded. We see that in its embracing\n",
      "of async/await... though the same can't be said of the ubiquitous Requests\n",
      "package. But the point almost still stands because httpx came along as an\n",
      "API-compatible version of Requests that does support async/await, so you don't\n",
      "really have to relearn old stuff. You can just focus on the new. And when\n",
      "Request finally does come around, chances are its implementation will be\n",
      "similar enough to httpx that your career of computer literacy won't be nuked\n",
      "the way it is in a move from React to Vue or from Vue to Svelte or whatnot.\n",
      "\n",
      "Okay, all that's just sort of a deep-breath preamble to what I want to do next\n",
      "on blogslicer. URLs are too long. I need to filter stop-words out of the URLs\n",
      "when a custom URL is constructed from the title. This is instead of\n",
      "implementing some arbitrary plug/slug replacement system. If you're basing URLs\n",
      "on titles then you're basing URLs on titles. Most of the things you'd want to\n",
      "overwrite that for is editing out stopwords anyway. And so...\n",
      "\n",
      "```python\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "nltk.download(\"stopwords\", quiet=True)\n",
      "stops = stopwords.words(\"english\")\n",
      "title = \" \".join([x for x in title.split() if x not in stops])\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### FILE: ../pythonically/_posts/2022-04-28-post-11.md"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'stops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 70>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table \u001b[38;5;129;01mand\u001b[39;00m maybe \u001b[38;5;129;01mand\u001b[39;00m maybe[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     89\u001b[0m     title \u001b[38;5;241m=\u001b[39m maybe[maybe\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\n\u001b[1;32m---> 90\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m title\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stops])\n\u001b[0;32m     91\u001b[0m     has_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     92\u001b[0m slug \u001b[38;5;241m=\u001b[39m slugify(title)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table \u001b[38;5;129;01mand\u001b[39;00m maybe \u001b[38;5;129;01mand\u001b[39;00m maybe[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     89\u001b[0m     title \u001b[38;5;241m=\u001b[39m maybe[maybe\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\n\u001b[1;32m---> 90\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m title\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstops\u001b[49m])\n\u001b[0;32m     91\u001b[0m     has_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     92\u001b[0m slug \u001b[38;5;241m=\u001b[39m slugify(title)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stops' is not defined"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import nltk\n",
    "import argparse\n",
    "from dateutil import parser\n",
    "from slugify import slugify\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "if hasattr(__builtins__, \"__IPYTHON__\"):\n",
    "    from IPython.display import display, Markdown\n",
    "\n",
    "    h1 = lambda text: display(Markdown(f\"# {text}\"))\n",
    "    h2 = lambda text: display(Markdown(f\"## {text}\"))\n",
    "    h3 = lambda text: display(Markdown(f\"### {text}\"))\n",
    "\n",
    "    folder_name = \"../pythonically\"\n",
    "    blog_title = \"Pythonic Ally Blog Index\"\n",
    "    blog_slug = \"blog\"\n",
    "else:\n",
    "    h1 = lambda text: print(f\"# {text}\")\n",
    "    h2 = lambda text: print(f\"## {text}\")\n",
    "    h3 = lambda text: print(f\"## {text}\")\n",
    "\n",
    "    aparser = argparse.ArgumentParser()\n",
    "    add_arg = aparser.add_argument\n",
    "    add_arg(\"-p\", \"--path\", required=True)\n",
    "    add_arg(\"-t\", \"--title\", required=True)\n",
    "    add_arg(\"-s\", \"--slug\", required=True)\n",
    "    args = aparser.parse_args()\n",
    "\n",
    "    folder_name = args.path\n",
    "    blog_title = args.title\n",
    "    blog_slug = args.slug\n",
    "\n",
    "\n",
    "index_front_matter = f\"\"\"---\n",
    "layout: post\n",
    "title: \"{blog_title}\"\n",
    "slug: {blog_slug}\n",
    "---\"\"\"\n",
    "\n",
    "journal_path = f\"{folder_name}/journal.md\"\n",
    "output_path = f\"{folder_name}/_posts/\"\n",
    "slicer = \"-\" * 80\n",
    "\n",
    "dates = []\n",
    "counter = -1\n",
    "date_next = False\n",
    "with open(journal_path, \"r\") as fh:\n",
    "    for line in fh:\n",
    "        line = line.rstrip()\n",
    "        if date_next:\n",
    "            adate = line[3:]\n",
    "            date_next = False\n",
    "            adatetime = parser.parse(adate).date()\n",
    "            dates.append(adatetime)\n",
    "            date_next = False\n",
    "        if line == slicer:\n",
    "            date_next = True\n",
    "            counter = counter + 1\n",
    "dates.reverse()\n",
    "\n",
    "h1(f\"There Are {counter} Posts:\")\n",
    "\n",
    "table = []\n",
    "at_top = True\n",
    "index_list = []\n",
    "with open(journal_path, \"r\") as fh:\n",
    "    for i, line in enumerate(fh):\n",
    "        line = line.rstrip()\n",
    "        if line == slicer:\n",
    "            if at_top:\n",
    "                at_top = False\n",
    "                table = []\n",
    "                continue\n",
    "            adatetime = dates[counter - 1]\n",
    "            filename = f\"{output_path}{adatetime}-post-{counter}.md\"\n",
    "            h3(f\"FILE: {filename}\")\n",
    "            with open(filename, \"w\") as fw:\n",
    "                title = f\"Post {counter}\"\n",
    "                slug = title\n",
    "                if table[0] == slicer:\n",
    "                    table = table[1:]\n",
    "                maybe = table[1]\n",
    "                has_title = False\n",
    "                if table and maybe and maybe[0] == \"#\":\n",
    "                    title = maybe[maybe.find(\" \") + 1 :]\n",
    "                    has_title = True\n",
    "                slug = \" \".join([x for x in title.split() if x not in stops])\n",
    "                slug = slugify(title)\n",
    "                top = []\n",
    "                top.append(\"---\\n\")\n",
    "                top.append(\"layout: post\\n\")\n",
    "                top.append(f'title: \"{title}\"\\n')\n",
    "                top.append(f\"categories: {blog_slug}\\n\")\n",
    "                top.append(f\"slug: {slug}\\n\")\n",
    "                link = f\"- [{title}](/{blog_slug}/{slug}/)\"\n",
    "                index_list.append(link)\n",
    "                top.append(\"---\\n\")\n",
    "                top.append(\"\\n\")\n",
    "                top_chop = 2\n",
    "                if has_title:\n",
    "                    top_chop = 3\n",
    "                table = [f\"{x}\\n\" for x in table[top_chop:]]\n",
    "                table = top + table\n",
    "                print(\"\".join(table))\n",
    "                fw.writelines(table)\n",
    "            counter = counter - 1\n",
    "            table = []\n",
    "        table.append(line)\n",
    "\n",
    "index_page = index_front_matter + \"\\n\\n\" + \"\\n\".join(index_list)\n",
    "\n",
    "with open(f\"{folder_name}/blog.md\", \"w\") as fh:\n",
    "    fh.writelines(index_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4bd40-e0f1-43ec-9340-d3038f591cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
